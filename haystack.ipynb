{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c91fd957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import document_stores\n",
    "from haystack.nodes import BM25Retriever, FARMReader\n",
    "from haystack.pipelines import ExtractiveQAPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f9a9229",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07ef59ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Using custom data configuration 20221201.simple-date=20221201,language=simple\n",
      "WARNING:datasets.builder:Found cached dataset wikipedia (/home/ec2-user/.cache/huggingface/datasets/olm___wikipedia/20221201.simple-date=20221201,language=simple/2.0.0/dbfec0358f063ec7ae9e247d6559e2e505fbce7463e666024718863cbf199ec6)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9db20e59fb746b895bcd00693bf04d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wiki_data = load_dataset(\"olm/wikipedia\", language='simple', date=\"20221201\", split=['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e069e1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.utils import launch_es\n",
    "launch_es()\n",
    "document_store = document_stores.ElasticsearchDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cdb8d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "rng = np.random.default_rng(12345)\n",
    "rand_idx = rng.random(50000)*len(wiki_data[0])\n",
    "wiki_small = pd.DataFrame(data=wiki_data[0][rand_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed13b46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Adenoids (also called pharyngeal tonsils, or nasopharyngeal tonsils) are tissues at the very back of the nose.  They are in the part of the nose where is joins the mouth.\\n\\nIn most children they make a soft bump on the top and back section of the nose's air passage.\\n\\nTaking away adenoids with surgery is called adenoidectomy.\\n\\nHead (body part)\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_small.iloc[0,:]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb8a38ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8bf35b557204785813bf0f09a1f65d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "getting wiki articles:   0%|          | 0/50000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "docs = []\n",
    "batch_size = 1000\n",
    "# total_doc_count = len(wiki_data)\n",
    "# total_doc_count = 50000\n",
    "# print('total_doc_count:', total_doc_count)\n",
    "for i in tqdm(range(len(wiki_small)), desc='getting wiki articles'):\n",
    "    to_dict = wiki_small.iloc[i,:]\n",
    "    doc = {'meta':{'id': to_dict['id'], 'url': to_dict['url'], 'title': to_dict['title']}}\n",
    "#     print('doc set up. adding text')\n",
    "    doc['content'] = to_dict['text'] \n",
    "    # doc = document_stores.convert_to_doc_dict(article)\n",
    "    docs.append(doc)\n",
    "    if i % batch_size == 0:     \n",
    "      document_store.write_documents(docs)      \n",
    "      docs.clear()\n",
    "#     if idx >= total_doc_count:\n",
    "#       break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69875c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = BM25Retriever(document_store=document_store)\n",
    "reader = FARMReader(model_name_or_path=\"deepset/roberta-base-squad2\", use_gpu=True)\n",
    "\n",
    "\n",
    "pipe = ExtractiveQAPipeline(reader, retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15493bd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.83s/ Batches\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'answer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4114/2787786352.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjeopardy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Retriever\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Reader\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'answer'"
     ]
    }
   ],
   "source": [
    "jeopardy = \"Whose theory did Galileo espouse and was under house arrest as a result for the last 8 years of his life?\"\n",
    "prediction = pipe.run(\n",
    "    query=jeopardy, params={\"Retriever\": {\"top_k\": 5}, \"Reader\": {\"top_k\": 1}}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6147fc0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Galileo Galilei'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction['answers'][0].to_dict()['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "833c6eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('JEOPARDY_QUESTIONS1.json','r') as file:\n",
    "    questions = json.load(file)\n",
    "    \n",
    "rand_q_idx = rng.random(1000)*39495\n",
    "sample_questions = []\n",
    "for i in rand_q_idx.astype('int'):\n",
    "    sample_questions.append(questions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "efffdc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample_questions.json','w') as file:\n",
    "    json.dump(sample_questions, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c62451b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'COMMON BONDS',\n",
       " 'air_date': '1998-09-07',\n",
       " 'question': \"'A game of footsie,<br />a bribe,<br />a drunk person'\",\n",
       " 'value': '$500',\n",
       " 'answer': 'Things that are done under the table',\n",
       " 'round': 'Jeopardy!',\n",
       " 'show_number': '3216'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e00508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fe3c0c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.82s/ Batches\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.16s/ Batches\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.72s/ Batches\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.77s/ Batches\n",
      "Inferencing Samples:   0%|        | 0/1 [00:01<?, ? Batches/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4114/588144160.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mthread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_pred_ans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_questions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjobs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4114/588144160.py\u001b[0m in \u001b[0;36mget_pred_ans\u001b[0;34m(sample_questions, id, score, ans)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_pred_ans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_questions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msample_questions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'category'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\": \"\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"Retriever\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Reader\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"top_k\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mpred_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answers'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_ans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/haystack/pipelines/standard_pipelines.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, query, params, debug)\u001b[0m\n\u001b[1;32m    336\u001b[0m                       \u001b[0mby\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0munder\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m\"_debug\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \"\"\"\n\u001b[0;32m--> 338\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/haystack/pipelines/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, query, file_paths, labels, documents, meta, params, debug)\u001b[0m\n\u001b[1;32m    522\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running node '%s` with input: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m                     \u001b[0mnode_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                     \u001b[0;31m# The input might be a really large object with thousands of embeddings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/haystack/pipelines/base.py\u001b[0m in \u001b[0;36m_run_node\u001b[0;34m(self, node_id, node_input)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    450\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode_input\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 451\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"component\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mnode_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     def run(  # type: ignore\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/haystack/nodes/base.py\u001b[0m in \u001b[0;36m_dispatch_run\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0mrun\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \"\"\"\n\u001b[0;32m--> 201\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch_run_general\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dispatch_run_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/haystack/nodes/base.py\u001b[0m in \u001b[0;36m_dispatch_run_general\u001b[0;34m(self, run_method, **kwargs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mrun_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mrun_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrun_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0;31m# Collect debug information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/haystack/nodes/reader/base.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, query, documents, top_k, labels, add_isolated_node_eval)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtiming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"query_time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtop_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"answers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/haystack/nodes/reader/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m             \u001b[0mtoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtoc\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/haystack/nodes/reader/farm.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, query, documents, top_k)\u001b[0m\n\u001b[1;32m    892\u001b[0m         \u001b[0;31m# get answers from QA model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m         \u001b[0;31m# TODO: Need fix in FARM's `to_dict` function of `QAInput` class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         predictions = self.inferencer.inference_from_objects(\n\u001b[0m\u001b[1;32m    895\u001b[0m             \u001b[0mobjects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiprocessing_chunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/haystack/modeling/infer.py\u001b[0m in \u001b[0;36minference_from_objects\u001b[0;34m(self, objects, return_json, multiprocessing_chunksize)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0;31m# then we can and should use inference from (input) objects!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0;31m# logger.warning(\"QAInferencer.inference_from_objects() will soon be deprecated. Use QAInferencer.inference_from_dicts() instead\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         return self.inference_from_dicts(\n\u001b[0m\u001b[1;32m    514\u001b[0m             \u001b[0mdicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiprocessing_chunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiprocessing_chunksize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/haystack/modeling/infer.py\u001b[0m in \u001b[0;36minference_from_dicts\u001b[0;34m(self, dicts, return_json, multiprocessing_chunksize)\u001b[0m\n\u001b[1;32m    479\u001b[0m                                     \u001b[0mhas\u001b[0m \u001b[0mbeen\u001b[0m \u001b[0mdeprecated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \"\"\"\n\u001b[0;32m--> 481\u001b[0;31m         return Inferencer.inference_from_dicts(\n\u001b[0m\u001b[1;32m    482\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_json\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmultiprocessing_chunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmultiprocessing_chunksize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m         )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/haystack/modeling/infer.py\u001b[0m in \u001b[0;36minference_from_dicts\u001b[0;34m(self, dicts, return_json, multiprocessing_chunksize)\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0maggregate_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_heads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"aggregate_preds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mpredictions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_without_multiprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maggregate_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/haystack/modeling/infer.py\u001b[0m in \u001b[0;36m_inference_without_multiprocessing\u001b[0;34m(self, dicts, return_json, aggregate_preds)\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# TODO change format of formatted_preds in QA (list of dicts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maggregate_preds\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0mpreds_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_predictions_and_aggregate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaskets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mpreds_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaskets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/haystack/modeling/infer.py\u001b[0m in \u001b[0;36m_get_predictions_and_aggregate\u001b[0;34m(self, dataset, tensor_names, baskets)\u001b[0m\n\u001b[1;32m    406\u001b[0m                 \u001b[0;31m# Aggregation works on preds, not logits. We want as much processing happening in one batch + on GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# So we transform logits to preds here as well\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m                 logits = self.model.forward(\n\u001b[0m\u001b[1;32m    409\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m                     \u001b[0msegment_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"segment_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/haystack/modeling/model/adaptive_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, segment_ids, padding_mask, output_hidden_states, output_attentions)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \"\"\"\n\u001b[1;32m    495\u001b[0m         \u001b[0;31m# Run forward pass of language model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         output_tuple = self.language_model.forward(\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             \u001b[0msegment_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/haystack/modeling/model/language_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, segment_ids, output_hidden_states, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_attentions\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_attentions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         )\n\u001b[0;32m--> 848\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m    849\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    522\u001b[0m                 )\n\u001b[1;32m    523\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    525\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0moutput_attentions\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m     ) -> Tuple[torch.Tensor]:\n\u001b[0;32m--> 336\u001b[0;31m         self_outputs = self.self(\n\u001b[0m\u001b[1;32m    337\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/transformers/models/roberta/modeling_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_layer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import threading\n",
    "def get_pred_ans(sample_questions, id, score, ans):\n",
    "    for q in sample_questions:\n",
    "        prediction = pipe.run(query = q['category'] +\": \"+ q['question'],params={\"Retriever\": {\"top_k\": 5}, \"Reader\": {\"top_k\": 1}})\n",
    "        pred_ans = prediction['answers'][0]\n",
    "        ans.append(pred_ans.to_dict()['answer'])\n",
    "        score.append(pred_ans.to_dict()['score'])\n",
    "threads = 4\n",
    "jobs = []\n",
    "for i in range(0, threads):\n",
    "    score = list()\n",
    "    ans = list()\n",
    "    thread = threading.Thread(target=get_pred_ans(sample_questions, i, score, ans))\n",
    "    jobs.append(thread)\n",
    "for j in jobs:\n",
    "    j.start()\n",
    "for j in jobs:\n",
    "    j.join()\n",
    "print(len(score))\n",
    "print(len(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ad96e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('sample_questions_with_predicted.json','r') as file:\n",
    "    sample_questions = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "633e30b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'category': 'COMMON BONDS', 'air_date': '1998-09-07', 'question': \"'A game of footsie,<br />a bribe,<br />a drunk person'\", 'value': '$500', 'answer': 'Things that are done under the table', 'round': 'Jeopardy!', 'show_number': '3216', 'predicted_question': [['question: What is the difference between a game of footsie,br />a bribe and a drunk person?']]}\n",
      "What is the difference between a game of footsie,br />a bribe and a drunk person?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Answer {'answer': 'Obi rejects', 'type': 'extractive', 'score': 0.07148900628089905, 'context': 'hips to students. A man offers a bribe on behalf of his sister, which Obi rejects. The girl herself visits Obi and tries to bribe him with sex for a s', 'offsets_in_document': [{'start': 1182, 'end': 1193}], 'offsets_in_context': [{'start': 70, 'end': 81}], 'document_id': 'a4614d69f5bebbed2c708de34bbc77df', 'meta': {'id': '775815', 'url': 'https://simple.wikipedia.org/wiki/No%20Longer%20at%20Ease', 'title': 'No Longer at Ease'}}>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = sample_questions[0]\n",
    "print(q)\n",
    "print(q['predicted_question'][0][0][10:])\n",
    "# prediction = pipe.run(query = q['predicted_question'][0][0][10:],params={\"Retriever\": {\"top_k\": 5}, \"Reader\": {\"top_k\": 1}})\n",
    "prediction['answers'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a910ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "300157a5754546dab02350325c3ab345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.03s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.39s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.14s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.18s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.66s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.52s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.40s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.31s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.52s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.41s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:17<00:00, 17.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.87s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:18<00:00, 18.33s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.34s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.45s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.34s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:15<00:00, 15.20s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.83s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.80s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.81s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.81s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.86s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.25s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.78s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.20s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.50s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.63s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.41s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.53s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.83s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.90s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.03s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.63s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.93s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.28s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.87s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.41s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.02s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:16<00:00, 16.53s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.44s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.76s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.20s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.44s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.22s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.02s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.89s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.42s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.20s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.21s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.76s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.30s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.54s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.64s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.04s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.82s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:28<00:00, 14.39s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.28s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.03s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.22s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.43s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.28s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.94s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.04s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.41s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.78s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.85s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.47s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.56s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.49s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.90s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.28s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:16<00:00, 16.48s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.10s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.46s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.66s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.65s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.67s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.26s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.65s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.53s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.64s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:15<00:00, 15.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.40s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.70s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.85s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.29s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.30s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.10s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.50s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.77s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.66s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.41s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.48s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.08s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.44s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.17s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:32<00:00, 16.42s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:15<00:00, 15.50s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.06s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.39s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.27s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.69s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.15s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.20s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.39s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.86s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.22s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.89s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.16s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.25s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.87s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:20<00:00, 20.82s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.51s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:17<00:00, 17.03s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.54s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.77s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.67s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.72s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:22<00:22, 22.93s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:32<00:00, 16.02s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.95s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.88s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.77s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.86s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:15<00:00, 15.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  3.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.14s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.55s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.92s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.10s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.95s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.19s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.95s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.46s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.10s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:15<00:00, 15.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.05s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.41s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.49s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.21s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:27<00:00, 13.85s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.86s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.07s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.56s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.14s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.92s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.95s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.15s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:18<00:00, 18.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.31s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:24<00:00, 12.03s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.96s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.08s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.19s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.96s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.21s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.95s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.19s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.48s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.49s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.67s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.42s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.22s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.42s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:16<00:00, 16.41s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.45s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.63s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.25s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.15s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.28s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:24<00:00, 12.20s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.48s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.15s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.39s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.86s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.45s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:17<00:00, 17.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.48s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:18<00:00, 18.30s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.56s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.86s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.33s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.40s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.52s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.56s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.96s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.46s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:16<00:00, 16.88s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.94s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.57s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:20<00:00, 20.02s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.92s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.14s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.21s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.55s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.22s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.53s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.95s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.49s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.80s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.96s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.21s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.49s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.55s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  3.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.15s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.88s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.53s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.85s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.06s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.96s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.95s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.96s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.10s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.84s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/3 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  33%|▎| 1/3 [00:22<00:45, 22.81s/ Batches\u001b[A\n",
      "Inferencing Samples:  67%|▋| 2/3 [00:45<00:22, 22.83s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 3/3 [00:47<00:00, 15.96s/ Batches\u001b[A\n",
      "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
      "(-31519, -30846) with a span answer. \n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:15<00:00, 15.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.91s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.02s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.96s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.22s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.50s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.49s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.54s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  3.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:23<00:00, 23.55s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:16<00:00, 16.03s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.76s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.47s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:15<00:00, 15.42s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.44s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:22<00:00, 23.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.16s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.14s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.14s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:20<00:00, 20.25s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.49s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.71s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.96s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.50s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.07s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:16<00:00, 16.90s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.86s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.16s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.83s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.84s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:22<00:00, 22.39s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.71s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.20s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.68s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.72s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.42s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.88s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.14s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.88s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.14s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.96s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.27s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.55s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.15s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.22s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:16<00:00, 16.50s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.56s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.51s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.81s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.22s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.25s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.25s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.18s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.92s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.89s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.22s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.55s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.88s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.85s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.71s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.03s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.22s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.51s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.15s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.72s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.55s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.84s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.57s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.29s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.83s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.51s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.29s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.64s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.86s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.65s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.22s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.10s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.85s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.93s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.42s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:25<00:00, 12.65s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.86s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.44s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:20<00:00, 20.91s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.76s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.48s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.85s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.19s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.03s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.31s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.27s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.19s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.88s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.84s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.26s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.88s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.47s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/3 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  33%|▎| 1/3 [00:23<00:46, 23.41s/ Batches\u001b[A\n",
      "Inferencing Samples:  67%|▋| 2/3 [00:46<00:23, 23.40s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 3/3 [01:00<00:00, 20.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.31s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:18<00:00, 18.88s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.70s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.28s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.96s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.91s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.34s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.54s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.66s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.49s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.96s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.48s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.53s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.14s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:17<00:00, 17.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:24<00:24, 24.08s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:26<00:00, 13.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.45s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.95s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.39s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.34s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.39s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:28<00:00, 14.04s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.64s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.02s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.34s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.48s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.07s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.04s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.03s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.39s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.64s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.03s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.87s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  3.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.27s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.96s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.63s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.56s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.40s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:27<00:00, 13.77s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.57s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.57s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.63s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:16<00:00, 16.04s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.51s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:17<00:00, 17.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:22<00:22, 22.60s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:24<00:00, 12.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.93s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  3.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.30s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.76s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.16s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.56s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.28s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.31s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.33s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.02s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.16s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.51s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.21s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.03s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.02s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.90s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:16<00:00, 16.48s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.08s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.25s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:17<00:00, 17.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:16<00:00, 16.50s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.54s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.55s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.54s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.45s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.43s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:28<00:00, 14.05s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  6.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.34s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.88s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.19s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.85s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.78s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.87s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.18s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.51s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.54s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.64s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.16s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.43s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.16s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.19s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.88s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.41s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:34<00:00, 17.50s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:20<00:00, 20.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.03s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.25s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.42s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:30<00:00, 15.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.18s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.09s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.57s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.55s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.94s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.85s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.53s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.29s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.04s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.42s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.90s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.02s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:17<00:00, 17.82s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.84s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.02s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.67s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:17<00:00, 17.80s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.19s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.14s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.46s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.45s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  3.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.55s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 12.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.87s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.85s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  8.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.30s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.33s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.56s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.41s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:26<00:00, 13.40s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.18s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.48s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.64s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.93s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:18<00:00, 18.34s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.56s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.87s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.42s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.18s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.18s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/3 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  33%|▎| 1/3 [00:23<00:46, 23.40s/ Batches\u001b[A\n",
      "Inferencing Samples:  67%|▋| 2/3 [00:46<00:23, 23.40s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 3/3 [00:47<00:00, 15.86s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.93s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:16<00:00, 16.48s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:17<00:00, 17.39s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.52s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.88s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.63s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.48s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:15<00:00, 15.07s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.88s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.20s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.16s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  8.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.54s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 12.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.87s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.57s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.86s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.47s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.26s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.19s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.57s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.33s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.14s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.14s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.88s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.92s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.18s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.89s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.33s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.52s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.96s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.88s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.47s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.22s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.15s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.14s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.78s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.55s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.43s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:31<00:00, 15.71s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.56s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.84s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.95s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.76s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.20s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.90s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  8.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.18s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:16<00:00, 16.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.82s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.52s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.14s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.25s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.34s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.50s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.31s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.52s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.18s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.31s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.92s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.50s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.87s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.91s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.14s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.18s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.45s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:25<00:00, 12.65s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.18s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.91s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.85s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:21<00:00, 21.96s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.89s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.21s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.86s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.44s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.50s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.46s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.87s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.90s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.40s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.49s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.76s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.02s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.50s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.85s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.43s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.42s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:32<00:00, 16.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.44s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.76s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.51s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.43s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:30<00:00, 15.22s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.63s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.86s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.49s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:16<00:00, 16.50s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.55s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.57s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.56s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:16<00:00, 16.06s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.34s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.03s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.43s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  3.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.63s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.09s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.02s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  6.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.51s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.66s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.06s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:40<00:00, 20.43s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.87s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.41s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.25s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.19s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.15s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.76s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.31s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.62s/ Batches\u001b[A\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "ans = []\n",
    "score = []\n",
    "timer = 0\n",
    "for i in tqdm(range(len(sample_questions))):\n",
    "    start = time.time()\n",
    "    q = sample_questions[i]\n",
    "    prediction = pipe.run(query = q['predicted_question'][0][0][10:],params={\"Retriever\": {\"top_k\": 5}, \"Reader\": {\"top_k\": 1}})\n",
    "    pred_ans = prediction['answers'][0]\n",
    "    timer += time.time()-start\n",
    "    ans.append(pred_ans.to_dict()['answer'])\n",
    "    score.append(pred_ans.to_dict()['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69c4a5d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.134882670879364"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timer/len(sample_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eba9dee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('haystack_ans.txt','r') as file:\n",
    "    og_ans = json.load(file)\n",
    "with open('haystack_score.txt','r') as file:\n",
    "    og_score = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bb33a008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Obi rejects'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8dd7eb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "    \n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    \n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "#https://qa.fastforwardlabs.com/no%20answer/null%20threshold/bert/distilbert/exact%20match/f1/robust%20predictions/2020/06/09/Evaluating_BERT_on_SQuAD.html#F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c078e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.051408561441512575\n",
      "0.034\n"
     ]
    }
   ],
   "source": [
    "f1s = 0\n",
    "ems = 0\n",
    "for idx, q in enumerate(sample_questions):\n",
    "    f1 = compute_f1(ans[idx], q['answer'])\n",
    "    em = compute_exact_match(ans[idx], q['answer'])\n",
    "    f1s += f1\n",
    "    ems += em\n",
    "print(f1s/len(ans))\n",
    "print(ems/len(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38b6bea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4840575724220811\n",
      "0.24649207638425363\n"
     ]
    }
   ],
   "source": [
    "f1s = 0\n",
    "ems = 0\n",
    "for idx, q in enumerate(sample_questions[:len(ans)]):\n",
    "    f1 = compute_f1(ans[idx], q['answer']) / score[idx]\n",
    "    em = compute_exact_match(ans[idx], q['answer']) / score[idx]\n",
    "    f1s += f1\n",
    "    ems += em\n",
    "print(f1s/len(ans))\n",
    "print(ems/len(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f9003ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('haystack_score_reform_q.txt','w') as file:\n",
    "    json.dump(score, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5b9d46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('haystack_ans_reform_q.txt','w') as file:\n",
    "    json.dump(ans, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aaef2566",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b664318ab2344fbfa2ef5fec130cacd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/281 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.16s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.13s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.31s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.87s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.20s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.84s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.30s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.32s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.45s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.63s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.42s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:31<00:00, 15.70s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.92s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.84s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.18s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:22<00:00, 22.96s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.86s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.92s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.68s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.29s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.87s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.04s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.50s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.43s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.64s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/3 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  33%|▎| 1/3 [00:23<00:46, 23.41s/ Batches\u001b[A\n",
      "Inferencing Samples:  67%|▋| 2/3 [00:46<00:23, 23.41s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 3/3 [00:48<00:00, 16.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:24<00:24, 24.80s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:27<00:00, 13.52s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.85s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.56s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.76s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.18s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.84s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.72s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:19<00:00, 19.42s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:18<00:00, 18.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.15s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.63s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.07s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.10s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.85s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.94s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.86s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.57s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.16s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.83s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:15<00:00, 15.49s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.84s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.96s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.28s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.90s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.16s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.89s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.49s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.49s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.92s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.77s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.93s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.20s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.29s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.86s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.33s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.16s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.51s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.09s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.19s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.77s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.92s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.64s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.88s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.63s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.55s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.92s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.37s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:46<00:00, 23.16s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.56s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.16s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.96s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.48s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.22s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.72s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:22<00:00, 22.96s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.42s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:15<00:00, 15.02s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.41s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.87s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.57s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.27s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.41s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:29<00:00, 14.52s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.57s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.27s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.84s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.56s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.84s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.72s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.41s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.35s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.15s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.92s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.36s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.16s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.05s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.41s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.86s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.63s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.57s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.14s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.50s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.15s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.99s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.19s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.40s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:31<00:00, 15.72s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.89s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.87s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:11<00:00, 11.12s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.20s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.86s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.88s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.63s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  8.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.46s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  6.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.15s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.20s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.54s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:18<00:00, 18.72s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:16<00:00, 16.06s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  6.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.98s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.22s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.84s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.45s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.11s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.30s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:21<00:00, 21.16s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.41s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.44s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.75s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.85s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.43s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.42s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.85s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:13<00:00, 13.30s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.76s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.40s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.41s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.84s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:16<00:00, 16.48s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.97s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.42s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.38s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.04s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.44s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:18<00:00, 18.66s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.65s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.23s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:12<00:00, 12.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.69s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  7.15s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.20s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.62s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:24<00:24, 24.71s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:25<00:00, 12.92s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.60s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.24s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:09<00:00,  9.80s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.95s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.52s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.61s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.20s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.84s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:08<00:00,  8.01s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:10<00:00, 10.67s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.22s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/2 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  50%|▌| 1/2 [00:23<00:23, 23.44s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 2/2 [00:24<00:00, 12.10s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:20<00:00, 20.54s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.59s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.73s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.64s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:16<00:00, 16.92s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.21s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.58s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:07<00:00,  8.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.95s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:06<00:00,  6.17s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  3.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/3 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples:  33%|▎| 1/3 [00:23<00:46, 23.42s/ Batches\u001b[A\n",
      "Inferencing Samples:  67%|▋| 2/3 [00:46<00:23, 23.42s/ Batches\u001b[A\n",
      "Inferencing Samples: 100%|█| 3/3 [00:50<00:00, 16.99s/ Batches\u001b[A\n",
      "ERROR:haystack.modeling.model.predictions:Invalid end offset: \n",
      "(-29301, -29291) with a span answer. \n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:01<00:00,  1.87s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.00s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:02<00:00,  2.22s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.74s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:03<00:00,  3.37s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:05<00:00,  5.56s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:04<00:00,  4.66s/ Batches\u001b[A\n",
      "\n",
      "Inferencing Samples:   0%|        | 0/1 [00:00<?, ? Batches/s]\u001b[A\n",
      "Inferencing Samples: 100%|█| 1/1 [00:14<00:00, 14.63s/ Batches\u001b[A\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "rest_ans = []\n",
    "rest_score = []\n",
    "timer = 0\n",
    "for i in tqdm(range(719,len(sample_questions))):\n",
    "    start = time.time()\n",
    "    q = sample_questions[i]\n",
    "    prediction = pipe.run(query = q['category'] +\": \"+ q['question'],params={\"Retriever\": {\"top_k\": 5}, \"Reader\": {\"top_k\": 1}})\n",
    "    pred_ans = prediction['answers'][0]\n",
    "    timer += time.time()-start\n",
    "    rest_ans.append(pred_ans.to_dict()['answer'])\n",
    "    rest_score.append(pred_ans.to_dict()['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd7aee4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_full_ans = np.append(og_ans, rest_ans)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e9590663",
   "metadata": {},
   "outputs": [],
   "source": [
    "og_full_score = np.append(og_score,rest_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9a0c00f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.045350003582828806\n",
      "0.03\n"
     ]
    }
   ],
   "source": [
    "f1s = 0\n",
    "ems = 0\n",
    "for idx, q in enumerate(sample_questions):\n",
    "    f1 = compute_f1(og_full_ans[idx], q['answer'])\n",
    "    em = compute_exact_match(og_full_ans[idx], q['answer'])\n",
    "    f1s += f1\n",
    "    ems += em\n",
    "print(f1s/len(ans))\n",
    "print(ems/len(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5be130bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7675834374679207\n",
      "0.534341242236703\n"
     ]
    }
   ],
   "source": [
    "f1s = 0\n",
    "ems = 0\n",
    "for idx, q in enumerate(sample_questions):\n",
    "    f1 = compute_f1(og_full_ans[idx], q['answer']) / og_full_score[idx]\n",
    "    em = compute_exact_match(og_full_ans[idx], q['answer']) / og_full_score[idx]\n",
    "    f1s += f1\n",
    "    ems += em\n",
    "print(f1s/len(ans))\n",
    "print(ems/len(ans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d52e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
